{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "60040932",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d6caf00c",
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes = load_diabetes()\n",
    "X = diabetes['data']\n",
    "y = diabetes['target']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, random_state = 42, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4f10e2bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_gradient_descend( X_train, y_train, epsilon, learning_rate, n_iteration, lambda_para):\n",
    "    n_train, n_feature = X_train.shape\n",
    "    weight = np.random.rand(n_feature)\n",
    "    b = np.random.rand()\n",
    "    \n",
    "    for _ in range(n_iteration):\n",
    "        for i in range(n_train):\n",
    "            if abs( y_train[i] - np.dot(weight, X_train[i]) + b) <= epsilon:\n",
    "                slope = lambda_para * weight * 2\n",
    "                weight -= learning_rate * slope\n",
    "            else:\n",
    "                if y_train[i] - np.dot(weight, X_train[i]) + b >=0:\n",
    "                    slope = lambda_para*weight*2 - X_train[i]\n",
    "                    weight -= learning_rate * slope\n",
    "                    b -= learning_rate \n",
    "                else:\n",
    "                    slope = lambda_para*weight*2 + X_train[i]\n",
    "                    weight -= learning_rate * slope\n",
    "                    b += learning_rate \n",
    "    return weight, b\n",
    "def predict(X_test, weight, b):\n",
    "    n_test = X_test.shape[0]\n",
    "    y_pred = np.zeros(n_test)\n",
    "    y_pred = np.dot(weight, X_test.T) - b\n",
    "    return y_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e27d13f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5399.453003060345\n",
      "26547.46084689926\n"
     ]
    }
   ],
   "source": [
    "weight, b = batch_gradient_descend(X_train, y_train, 0.1, 0.01, 100, 0.1)\n",
    "y_pred = predict(X_test, weight, b)\n",
    "#print(y_pred)\n",
    "#print(y_test)\n",
    "cost = 1/len(y_test) * np.sum( np.dot(y_pred - y_test, (y_pred - y_test).T))\n",
    "print(cost)\n",
    "lnr = LinearRegression()\n",
    "lnr.fit(X_train, y_train)\n",
    "para = lnr.coef_\n",
    "cost_lnr = 0\n",
    "for i in range(len(y_test)):\n",
    "    cost_lnr += 1/len(y_test)*( np.dot(X_test[i], weight) - y_test[i])**2\n",
    "print(cost_lnr)\n",
    "# better than linear regression"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
